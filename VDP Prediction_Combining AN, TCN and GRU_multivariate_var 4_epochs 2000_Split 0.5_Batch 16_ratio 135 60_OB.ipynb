{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIT License\n",
    "\n",
    "#Copyright (c) [2021] [Oliver Böhme]\n",
    "\n",
    "#Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#of this software and associated documentation files (the \"Software\"), to deal\n",
    "#in the Software without restriction, including without limitation the rights\n",
    "#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#copies of the Software, and to permit persons to whom the Software is\n",
    "#furnished to do so, subject to the following conditions:\n",
    "\n",
    "#The above copyright notice and this permission notice shall be included in all\n",
    "#copies or substantial portions of the Software.\n",
    "\n",
    "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "#SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem Framing:\n",
    "\n",
    "#There are many ways to harness and explore a given dataset.\n",
    "#In this paper, we will use the data to explore a very specific question; that is:\n",
    "#Given recent number of problems and influencal factors what is the expected number of problems for the upcoming time steps until we reach SOP?\n",
    "\n",
    "#This requires that a predictive model forecasts the total number of errors for each week over the residual weeks.\n",
    "\n",
    "#Technically, this framing of the problem is referred to as a multivariate multi-step time series forecasting problem, given the multiple forecast steps. \n",
    "#A model, that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model.\n",
    "#A model of this type could be helpful within the <Anwendungsfall>\n",
    "#It could also be helpful on the <Anwendungsfall>.\n",
    "\n",
    "#Data Description\n",
    "#Non-stationarity is when the statistical properties of a series, e.g the mean, variance, and covariance (or the process generating the series) changes over time. \n",
    "#Non-stationary series are typically difficult to model and forecast and are therefore required to be made stationary to obtain meaningful results as many statistical tools and processes require stationarity. \n",
    "#A proven method of stationarizing a non-stationary series is through the use of differencing.\n",
    "\n",
    "#Credits\n",
    "#https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries:\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import to_numeric\n",
    "\n",
    "import numpy as np\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "\n",
    "import csv\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, LSTM, Dense, GRU, Flatten, RepeatVector, TimeDistributed\n",
    "from tcn import TCN, tcn_full_summary #https://github.com/philipperemy/keras-tcn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import DataSet:\n",
    "\n",
    "data_raw = pd.read_excel('S:/03_Promotion/03_Veröffentlichungen/02_Paper/04_Prediction/Experiment/DataSet_Prediction_multivariat_preprocessed_feature selected_v04e_20211003_OB.xlsm',0)\n",
    "print('Shape of the Dataset_raw: ',data_raw.shape)\n",
    "data_raw.head()\n",
    "\n",
    "#Drop unneccessary Columns:\n",
    "dataset = data_raw.iloc[:,3:11]\n",
    "\n",
    "#Plot the Data\n",
    "figure, axis = plt.subplots(2,4, figsize=(15,6), constrained_layout=True)\n",
    "\n",
    "axis[0,0].plot(dataset.iloc[:,0:1])\n",
    "axis[0,0].set_title(\"# offene Gesamtfehler\")\n",
    "\n",
    "axis[0,1].plot(dataset.iloc[:,1:2])\n",
    "axis[0,1].set_title(\"# Projekte parallel\")\n",
    "\n",
    "axis[0,2].plot(dataset.iloc[:,2:3])\n",
    "axis[0,2].set_title(\"Ø MA/Projekt\")\n",
    "\n",
    "axis[0,3].plot(dataset.iloc[:,3:4])\n",
    "axis[0,3].set_title(\"MA-Zufriedenheit\")\n",
    "\n",
    "axis[1,0].plot(dataset.iloc[:,4:5])\n",
    "axis[1,0].set_title(\"Neue Fzg.-Architektur\")\n",
    "\n",
    "axis[1,1].plot(dataset.iloc[:,5:6])\n",
    "axis[1,1].set_title(\"Ersteinsetzer\")\n",
    "\n",
    "axis[1,2].plot(dataset.iloc[:,6:7])\n",
    "axis[1,2].set_title(\"Antriebsart\")\n",
    "\n",
    "axis[1,3].plot(dataset.iloc[:,7:8])\n",
    "axis[1,3].set_title(\"Eigenleistungsquote\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot offene Gesamtfehler\n",
    "projects2visualize = dataset\n",
    "time = np.arange(len(projects2visualize))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Number of errors\")\n",
    "    \n",
    "for i in range(0,256):\n",
    "    plt.plot(time[0:195], projects2visualize.iloc[i*195:((i*195)+195),0:1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose Feature Set:\n",
    "dataset = pd.concat([data_raw.iloc[:,3:4], data_raw.iloc[:,4:5], data_raw.iloc[:,5:6],data_raw.iloc[:,10:11]], axis=1)\n",
    "print('Shape of the Dataset: ',dataset.shape)\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Length of Input/Output Sequences:\n",
    "\n",
    "n_input = 135\n",
    "n_out = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split multivariate DataSet into Train/ Test Sets:\n",
    "\n",
    "#Normalize Dataset\n",
    "#scaler = MinMaxScaler()\n",
    "#dataset_scaled = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "#Split into Length of Car Projects (195 time stamps)\n",
    "split_time = math.ceil(0.5*len(dataset)/195)*195 #Ratio: 70/30 #Define Ratio Train/Test-Split\n",
    "train, test = dataset[:split_time], dataset[split_time:]\n",
    "#train, test = dataset_scaled[:split_time], dataset_scaled[split_time:] #Normalize Data\n",
    "    \n",
    "#Restructure into Windows of Length of one Verhicle Project\n",
    "train = array(split(train, len(train)/195))\n",
    "test = array(split(test, len(test)/195))\n",
    "\n",
    "\n",
    "#Testing/Bugfixing\n",
    "print(\"Train Shape: \", train.shape)\n",
    "print(\"Test Shape: \", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation: Convert Train-DataSet into Inputs and Outputs:\n",
    "\n",
    "#Flatten Data\n",
    "data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\n",
    "X, y = list(), list()\n",
    "in_start = 0\n",
    "    \n",
    "#Step over the entire History one time step at a time\n",
    "for i in range(len(data)):\n",
    "    #Define the End of the Input/Output Sequence\n",
    "    in_end = in_start + n_input\n",
    "    out_end = in_end + n_out\n",
    "        \n",
    "    #Ensure to have enough Data for this Instance\n",
    "    if out_end <= len(data):\n",
    "        X_input = data[in_start:in_end, 0]\n",
    "        X_input = X_input.reshape((len(X_input), 1))\n",
    "        X.append(data[in_start:in_end, :])\n",
    "        y.append(data[in_end:out_end, 0])\n",
    "        \n",
    "    #Move along one time step\n",
    "    in_start += 195\n",
    "\n",
    "train_y = array(y)\n",
    "#Debug\n",
    "print(array(X).shape)\n",
    "print(array(y).shape)\n",
    "#print(array(X)[0:330])\n",
    "#print(array(y)[0:330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation: Convert Test-DataSet into Inputs and Outputs:\n",
    "\n",
    "#Flatten Data\n",
    "data = test.reshape((test.shape[0]*test.shape[1], test.shape[2]))\n",
    "\n",
    "X_test, y_test = list(), list()\n",
    "in_start = 0\n",
    "    \n",
    "#Step over the entire History one time step at a time\n",
    "for i in range(len(data)):\n",
    "    #Define the End of the Input/Output Sequence\n",
    "    in_end = in_start + n_input\n",
    "    out_end = in_end + n_out\n",
    "        \n",
    "    #Ensure to have enough Data for this Instance\n",
    "    if out_end <= len(data):\n",
    "        X_test_input = data[in_start:in_end, 0]\n",
    "        X_test_input = X_test_input.reshape((len(X_test_input), 1))\n",
    "        X_test.append(data[in_start:in_end, :])\n",
    "        y_test.append(data[in_end:out_end, 0])\n",
    "        \n",
    "    #Move along one time step\n",
    "    in_start += 195\n",
    "\n",
    "X_test = array(X_test)\n",
    "y_test = array(y_test)\n",
    "#Debug\n",
    "#print(array(X_test).shape)\n",
    "#print(array(y_test).shape)\n",
    "#print(array(X_test)[0:330])\n",
    "#print(array(y_test)[0:330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define and train the AN_Model:\n",
    "\n",
    "#Prepare Data\n",
    "AN_train_x, AN_train_y = array(X), array(y)\n",
    "\n",
    "#Define Parameters\n",
    "verbose, epochs, batch_size = 1, 2000, 16\n",
    "n_timesteps, n_features, n_outputs = AN_train_x.shape[1], AN_train_x.shape[2], AN_train_y.shape[1]\n",
    "#print(\"n_timesteps, n_features, n_outputs: \",n_timesteps, n_features, n_outputs)\n",
    "\n",
    "#Reshape Output into [samples, timesteps, features]\n",
    "AN_train_y = AN_train_y.reshape((AN_train_y.shape[0], AN_train_y.shape[1], 1)) \n",
    "    \n",
    "#Define Transformer Encoder\n",
    "#Credits: https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    #Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    #Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "#Define Build Model Function\n",
    "#Credits: https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "        \n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(units=n_out, activation=\"relu\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "#Define Model\n",
    "AN_model = build_model(input_shape=(n_timesteps, n_features), head_size=256, num_heads=1, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25)\n",
    "\n",
    "AN_model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#Fit the Model\n",
    "AN_model.fit(AN_train_x, AN_train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and train the TCN_Model:\n",
    "\n",
    "#Prepare Data\n",
    "TCN_train_x, TCN_train_y = array(X), array(y)\n",
    "\n",
    "#Define Parameters\n",
    "verbose, epochs, batch_size = 1, 2000, 16\n",
    "n_timesteps, n_features, n_outputs = TCN_train_x.shape[1], TCN_train_x.shape[2], TCN_train_y.shape[1]\n",
    "#print(\"n_timesteps, n_features, n_outputs: \",n_timesteps, n_features, n_outputs)\n",
    "\n",
    "#Reshape Output into [samples, timesteps, features]\n",
    "TCN_train_y = TCN_train_y.reshape((TCN_train_y.shape[0], TCN_train_y.shape[1], 1)) \n",
    "\n",
    "#Define Model\n",
    "TCN_model = Sequential()\n",
    "\n",
    "TCN_model.add(TCN(nb_filters=256, nb_stacks=1, dropout_rate=0.0, return_sequences=False, activation='tanh',input_shape=(n_timesteps, n_features)))\n",
    "\n",
    "TCN_model.add(RepeatVector(n_out))\n",
    "\n",
    "TCN_model.add(TCN(nb_filters=256, nb_stacks=1, dropout_rate=0.0, return_sequences=True, activation='tanh'))\n",
    "\n",
    "TCN_model.add(TimeDistributed(Dense(128, activation='relu'))) #We will then use a fully connected layer to interpret each time step in the output sequence before the final output layer.\n",
    "\n",
    "TCN_model.add(TimeDistributed(Dense(1)))\n",
    "    \n",
    "TCN_model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#Fit the Model\n",
    "TCN_model.fit(TCN_train_x, TCN_train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and train the GRU_Model:\n",
    "\n",
    "#Prepare Data\n",
    "GRU_train_x, GRU_train_y = array(X), array(y)\n",
    "\n",
    "#Define Parameters\n",
    "verbose, epochs, batch_size = 1, 2000, 16\n",
    "n_timesteps, n_features, n_outputs = GRU_train_x.shape[1], GRU_train_x.shape[2], GRU_train_y.shape[1]\n",
    "#print(\"n_timesteps, n_features, n_outputs: \",n_timesteps, n_features, n_outputs)\n",
    "\n",
    "#Reshape Output into [samples, timesteps, features]\n",
    "GRU_train_y = GRU_train_y.reshape((GRU_train_y.shape[0], GRU_train_y.shape[1], 1)) \n",
    "    \n",
    "#Define Model\n",
    "GRU_model = Sequential()\n",
    "\n",
    "GRU_model.add(GRU(1024, activation='tanh', input_shape=(n_timesteps, n_features)))\n",
    "\n",
    "GRU_model.add(RepeatVector(n_out)) #The Internal representation of the input sequence is repeated multiple times, once for each time step in the output sequence\n",
    "\n",
    "GRU_model.add(GRU(1024, activation='tanh', return_sequences=True)) #We then define the decoder as an LSTM hidden layer with 200 units. Importantly, the decoder will output the entire sequence, not just the output at the end of the sequence as we did with the encoder. This means that each of the 200 units will output a value for each of the seven days, representing the basis for what to predict for each day in the output sequence.\n",
    "    \n",
    "GRU_model.add(TimeDistributed(Dense(512, activation='relu'))) #We will then use a fully connected layer to interpret each time step in the output sequence before the final output layer.\n",
    "    \n",
    "GRU_model.add(TimeDistributed(Dense(1))) #Importantly, the output layer predicts a single step in the output sequence, not all seven days at a time. The network therefore outputs a three-dimensional vector with the same structure as the input, with the dimensions [samples, timesteps, features]\n",
    "    \n",
    "GRU_model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#Fit the Model\n",
    "GRU_model.fit(GRU_train_x, GRU_train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Forecasts:\n",
    "print(\"Predict the last \",n_out,\" Values of the last Vehicle Development Project:\\n\")\n",
    "#Flatten Data\n",
    "history = [x for x in train] #Note: History = train\n",
    "data = array(history)\n",
    "data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "#print(\"Shape History: \",data.shape)\n",
    "\n",
    "#Retrieve last Observations for Input Data\n",
    "input_x = data[-n_input-n_out:-n_out, :]\n",
    "#print(input_x.shape)\n",
    "\n",
    "#Reshape into [1, n_input, n]\n",
    "input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1])) #multivariate\n",
    "#print(\"Shape input_x: \",input_x.shape)\n",
    "\n",
    "#Forecast\n",
    "AN_yhat = AN_model.predict(input_x, verbose=0)\n",
    "TCN_yhat = TCN_model.predict(input_x, verbose=0)\n",
    "GRU_yhat = GRU_model.predict(input_x, verbose=0)\n",
    "    \n",
    "#We only want the Vector Forecast\n",
    "AN_yhat = AN_yhat[0]\n",
    "TCN_yhat = TCN_yhat[0]\n",
    "GRU_yhat = GRU_yhat[0]\n",
    "#print(\"Length yhat: \",len(yhat),\"\\n\")\n",
    "#print(\"Array yhat: \",yhat)\n",
    "\n",
    "\n",
    "#Vergleich y_train mit yhat im letzten Trainingsbeispiel\n",
    "X = input_x.reshape((input_x.shape[0]*input_x.shape[1], input_x.shape[2]))[:,0]\n",
    "y = train_y.reshape((train_y.shape[0]*train_y.shape[1], train_y.shape[2]))[-n_out:]\n",
    "time = np.arange(len(dataset))\n",
    "\n",
    "#Plot Data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Example 1\")\n",
    "\n",
    "plt.plot(time[0:135], X[:])\n",
    "plt.plot(time[135:195], y[:])\n",
    "plt.plot(time[135:195], AN_yhat[:])\n",
    "plt.plot(time[135:195], TCN_yhat[:])\n",
    "plt.plot(time[135:195], GRU_yhat[:])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Model:\n",
    "\n",
    "AN_predictions = list()\n",
    "TCN_predictions = list()\n",
    "GRU_predictions = list()\n",
    "\n",
    "#print(\"Test Shape: \",test.shape)\n",
    "for i in range(test.shape[0]):\n",
    "    #Predict the Output Sequence\n",
    "    history = test[i:i+1,:,:] #for each Car Project\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "        \n",
    "    #Retrieve last Observations for Input Data\n",
    "    input_x = data[-n_input-n_out:-n_out, :]\n",
    "    \n",
    "    #Reshape into [1, n_input, n]\n",
    "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1])) #multivariate\n",
    "    \n",
    "    #Forecast\n",
    "    AN_yhat = AN_model.predict(input_x, verbose=0)\n",
    "    TCN_yhat = TCN_model.predict(input_x, verbose=0)\n",
    "    GRU_yhat = GRU_model.predict(input_x, verbose=0)\n",
    "    \n",
    "    #We only want the Vector Forecast\n",
    "    AN_yhat_sequence = AN_yhat[0]\n",
    "    TCN_yhat_sequence = TCN_yhat[0]\n",
    "    GRU_yhat_sequence = GRU_yhat[0]\n",
    "        \n",
    "    #Store the Predictions\n",
    "    AN_predictions.append(AN_yhat_sequence)\n",
    "    TCN_predictions.append(TCN_yhat_sequence)\n",
    "    GRU_predictions.append(GRU_yhat_sequence)\n",
    "        \n",
    "#Evaluate predicted Time Steps for each Car Project\n",
    "AN_predictions = array(AN_predictions)\n",
    "TCN_predictions = array(TCN_predictions)\n",
    "GRU_predictions = array(GRU_predictions)\n",
    "\n",
    "AN_scores = list()\n",
    "TCN_scores = list()\n",
    "GRU_scores = list()\n",
    "\n",
    "actual, AN_predicted = array(y_test), array(AN_predictions)\n",
    "actual, TCN_predicted = array(y_test), array(TCN_predictions)\n",
    "actual, GRU_predicted = array(y_test), array(GRU_predictions)\n",
    "\n",
    "\n",
    "for i in range(AN_predicted.shape[1]):\n",
    "    #Calculate MSE\n",
    "    #mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "    \n",
    "    #Calculate RMSE\n",
    "    #rmse = sqrt(mse)\n",
    "    \n",
    "    #Calculate MAE\n",
    "    AN_mae = mean_absolute_error(actual[:, i], AN_predicted[:, i])\n",
    "    TCN_mae = mean_absolute_error(actual[:, i], TCN_predicted[:, i])\n",
    "    GRU_mae = mean_absolute_error(actual[:, i], GRU_predicted[:, i])\n",
    "    #print(\"Step: \",i)\n",
    "    #print(\"y\", actual[:,i])\n",
    "    #print(\"yhat\", predicted[:,i])\n",
    "    #print(\"MAE: \", mae)\n",
    "    \n",
    "    #Store the Scores\n",
    "    #scores.append(rmse)\n",
    "    AN_scores.append(AN_mae)\n",
    "    TCN_scores.append(TCN_mae)\n",
    "    GRU_scores.append(GRU_mae)\n",
    "\n",
    "#Calculate overall MAE (alternative: RMSE)\n",
    "AN_s = 0\n",
    "TCN_s = 0\n",
    "GRU_s = 0\n",
    "    \n",
    "for row in range(AN_predicted.shape[0]):\n",
    "    for col in range(AN_predicted.shape[1]):\n",
    "        AN_s += (actual[row, col] - AN_predicted[row, col])**2\n",
    "        TCN_s += (actual[row, col] - TCN_predicted[row, col])**2\n",
    "        GRU_s += (actual[row, col] - GRU_predicted[row, col])**2\n",
    "    \n",
    "AN_score = sqrt(AN_s / (actual.shape[0] * actual.shape[1]))\n",
    "TCN_score = sqrt(TCN_s / (actual.shape[0] * actual.shape[1]))\n",
    "GRU_score = sqrt(GRU_s / (actual.shape[0] * actual.shape[1]))\n",
    "\n",
    "#Visualize AN_MAE\n",
    "AN_s_scores = ', '.join(['%.1f' % AN_s for AN_s in AN_scores])\n",
    "print('%s: [%.3f] %s' % ('Multi-Head Attention Network', AN_score, AN_s_scores))\n",
    "pyplot.plot(AN_scores, marker='o', label='AN')\n",
    "pyplot.show()\n",
    "print(\"Overall MAE: \",AN_score)\n",
    "\n",
    "#Visualize TCN_MAE\n",
    "TCN_s_scores = ', '.join(['%.1f' % TCN_s for TCN_s in TCN_scores])\n",
    "print('%s: [%.3f] %s' % ('Temporal Convolutional Network', TCN_score, TCN_s_scores))\n",
    "pyplot.plot(TCN_scores, marker='o', label='TCN')\n",
    "pyplot.show()\n",
    "print(\"Overall MAE: \",TCN_score)\n",
    "\n",
    "#Visualize GRU_MAE\n",
    "GRU_s_scores = ', '.join(['%.1f' % GRU_s for GRU_s in GRU_scores])\n",
    "print('%s: [%.3f] %s' % ('GRU Neural Network', GRU_score, GRU_s_scores))\n",
    "pyplot.plot(GRU_scores, marker='o', label='GRU')\n",
    "pyplot.show()\n",
    "print(\"Overall MAE: \",GRU_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "########################################## Visualization of Predicted Data ################################################\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Predictions Test-Set/Predictive-Set\n",
    "#Define Data Range - Example 1\n",
    "example=1\n",
    "\n",
    "X_test_data=X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
    "X_test_data=X_test_data[:,0:1]\n",
    "y_test_data=y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
    "#print(X_test_data.shape)\n",
    "AN_prediction_data=AN_predicted.reshape((AN_predicted.shape[0]*AN_predicted.shape[1]))\n",
    "TCN_prediction_data=TCN_predicted.reshape((TCN_predicted.shape[0]*TCN_predicted.shape[1]))\n",
    "GRU_prediction_data=GRU_predicted.reshape((GRU_predicted.shape[0]*GRU_predicted.shape[1]))\n",
    "time = np.arange(len(dataset))\n",
    "\n",
    "#Plot Data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Example \"+str(example))\n",
    "\n",
    "plt.plot(time[0:135], X_test_data[(example-1)*n_input:example*n_input])\n",
    "plt.plot(time[135:195], y_test_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], AN_prediction_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], TCN_prediction_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], GRU_prediction_data[(example-1)*n_out:example*n_out])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Predictions Test-Set/Predictive-Set\n",
    "#Define Data Range - Example 2\n",
    "example=2\n",
    "X_test_data=X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
    "X_test_data=X_test_data[:,0:1]\n",
    "y_test_data=y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
    "#print(X_test_data.shape)\n",
    "AN_prediction_data=AN_predicted.reshape((AN_predicted.shape[0]*AN_predicted.shape[1]))\n",
    "TCN_prediction_data=TCN_predicted.reshape((TCN_predicted.shape[0]*TCN_predicted.shape[1]))\n",
    "GRU_prediction_data=GRU_predicted.reshape((GRU_predicted.shape[0]*GRU_predicted.shape[1]))\n",
    "time = np.arange(len(dataset))\n",
    "\n",
    "#Plot Data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Example \"+str(example))\n",
    "\n",
    "plt.plot(time[0:135], X_test_data[(example-1)*n_input:example*n_input])\n",
    "plt.plot(time[135:195], y_test_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], AN_prediction_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], TCN_prediction_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], GRU_prediction_data[(example-1)*n_out:example*n_out])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Predictions Test-Set/Predictive-Set\n",
    "#Define Data Range - Example 3\n",
    "example=30\n",
    "X_test_data=X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
    "X_test_data=X_test_data[:,0:1]\n",
    "y_test_data=y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
    "#print(X_test_data.shape)\n",
    "AN_prediction_data=AN_predicted.reshape((AN_predicted.shape[0]*AN_predicted.shape[1]))\n",
    "TCN_prediction_data=TCN_predicted.reshape((TCN_predicted.shape[0]*TCN_predicted.shape[1]))\n",
    "GRU_prediction_data=GRU_predicted.reshape((GRU_predicted.shape[0]*GRU_predicted.shape[1]))\n",
    "time = np.arange(len(dataset))\n",
    "\n",
    "#Plot Data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Example \"+str(example))\n",
    "\n",
    "plt.plot(time[0:135], X_test_data[(example-1)*n_input:example*n_input])\n",
    "plt.plot(time[135:195], y_test_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], AN_prediction_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], TCN_prediction_data[(example-1)*n_out:example*n_out])\n",
    "plt.plot(time[135:195], GRU_prediction_data[(example-1)*n_out:example*n_out])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "########################################## Verifikation am realen Beispiel ################################################\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Validation Data\n",
    "validation_data = pd.read_excel('S:/03_Promotion/03_Veröffentlichungen/02_Paper/04_Prediction/Experiment/X_Test_Prediction_multivariat_preprocessed_v03e_20211003_OB.xlsm',0)\n",
    "#print('Shape des RAW-Datasets: ',validation_data.shape)\n",
    "validation_data.head()\n",
    "\n",
    "#Drop unneccessary Columns:\n",
    "validation_data = validation_data.iloc[:,3:5]\n",
    "#print('Shape of the Dataset: ',validation_data.shape)\n",
    "#print(validation_data.head())\n",
    "\n",
    "#Normalize Input Data\n",
    "#scaler = MinMaxScaler()\n",
    "#validation_data_scaled = scaler.fit_transform(validation_data)\n",
    "\n",
    "#Set nth sample to show\n",
    "n=3\n",
    "\n",
    "#Manuelle Definition Array erstes reales Verifikationsbeispiel x1 \n",
    "#x1 = np.array(validation_data_scaled[(n-1)*195:n*195,:]) #normalized\n",
    "x1 = np.array(validation_data.iloc[(n-1)*195:n*195,:])\n",
    "x1 = array(split(x1, len(x1)/195))\n",
    "#print(\"Shape x1: \\n\",x1.shape,\"\\n\")\n",
    "#print(\"x1: \\n\",x1)\n",
    "\n",
    "#Make Forecasts:\n",
    "print(\"Predict the last 60 Values of the test vehicle project:\\n\")\n",
    "#Flatten Data\n",
    "history = x1 #Note: History = x1\n",
    "data = array(history)\n",
    "data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "#print(\"Shape History: \",data.shape)\n",
    "\n",
    "#Retrieve last Observations for Input Data\n",
    "input_x = data[-n_input-n_out:-n_out, :]\n",
    "#print(input_x.shape)\n",
    "\n",
    "#Reshape into [1, n_input, n]\n",
    "input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1])) #multivariate\n",
    "#print(\"Shape input_x: \",input_x.shape)\n",
    "\n",
    "#Forecast\n",
    "AN_yhat = AN_model.predict(input_x, verbose=0)\n",
    "TCN_yhat = TCN_model.predict(input_x, verbose=0)\n",
    "GRU_yhat = GRU_model.predict(input_x, verbose=0)\n",
    "    \n",
    "#We only want the Vector Forecast\n",
    "AN_yhat = AN_yhat[0]\n",
    "TCN_yhat = TCN_yhat[0]\n",
    "GRU_yhat = GRU_yhat[0]\n",
    "#print(\"Length yhat: \",len(yhat),\"\\n\")\n",
    "#print(\"Array yhat: \",yhat)\n",
    "\n",
    "\n",
    "#Vergleich y_train mit yhat im letzten Trainingsbeispiel\n",
    "X = input_x.reshape((input_x.shape[0]*input_x.shape[1], input_x.shape[2]))[:,0]\n",
    "time = np.arange(len(dataset))\n",
    "\n",
    "#Plot Data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Forecast Example \"+str(n))\n",
    "\n",
    "plt.plot(time[0:135], X[:])\n",
    "plt.plot(time[135:195], AN_yhat[:])\n",
    "#plt.plot(time[135:195], TCN_yhat[:])\n",
    "#plt.plot(time[135:195], GRU_yhat[:])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Forecast Integral\n",
    "#find max, min and average value vectors \n",
    "max, min, avg = list(), list(), list()\n",
    "for i in range(len(AN_yhat)):        \n",
    "    if AN_yhat[i] > TCN_yhat[i]:\n",
    "        max.append(AN_yhat[i])\n",
    "    else: max.append(TCN_yhat[i])\n",
    "\n",
    "for i in range(len(AN_yhat)):        \n",
    "    if AN_yhat[i] < TCN_yhat[i]:\n",
    "        min.append(AN_yhat[i])\n",
    "    else: min.append(TCN_yhat[i])\n",
    "\n",
    "for i in range(len(AN_yhat)):        \n",
    "    avg_value = 1/2*(AN_yhat[i]+TCN_yhat[i])\n",
    "    avg.append(avg_value)\n",
    "        \n",
    "#Plot Data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Errors\")\n",
    "plt.title(\"Forecast Example \"+str(n))\n",
    "\n",
    "plt.plot(time[0:135], X[:])\n",
    "#plt.plot(time[135:195], max[:],color='orange')\n",
    "#plt.plot(time[135:195], min[:],color='orange')\n",
    "plt.plot(time[135:195], avg[:],color='darkblue',marker='.')\n",
    "plt.fill_between(time[135:195],np.reshape(np.array(max),-1), np.reshape(np.array(min),-1),color='lightgrey')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
